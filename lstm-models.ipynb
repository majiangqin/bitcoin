{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 9472352,
     "sourceType": "datasetVersion",
     "datasetId": 5760569
    }
   ],
   "dockerImageVersionId": 30776,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Define Evaluation Metrics\n\nSince  predicting a continuous variable (block_median_fee_rate), regression metrics like:\n\nMean Absolute Error (MAE)\n\nRoot Mean Squared Error (RMSE)\n\nR² Score",
   "metadata": {
    "id": "95f0e2374ece7a7c"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Neural net works- LSTM",
   "metadata": {
    "id": "c557c1505e29fe56"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install scikeras\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:42:08.716072Z",
     "start_time": "2024-09-22T18:42:07.855586Z"
    },
    "id": "f3ff4c4ca0ec47fc",
    "outputId": "70def506-1142-48dc-d656-fe4f657e11b6",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:05:30.329210Z",
     "iopub.execute_input": "2024-09-28T03:05:30.329535Z",
     "iopub.status.idle": "2024-09-28T03:05:48.809575Z",
     "shell.execute_reply.started": "2024-09-28T03:05:30.329490Z",
     "shell.execute_reply": "2024-09-28T03:05:48.808331Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting scikeras\n  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from scikeras) (3.3.3)\nCollecting scikit-learn>=1.4.2 (from scikeras)\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (3.11.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.3.2)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.1)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\nDownloading scikeras-0.13.0-py3-none-any.whl (26 kB)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.3/13.3 MB\u001B[0m \u001B[31m54.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: scikit-learn, scikeras\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed scikeras-0.13.0 scikit-learn-1.5.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# from google.colab import drive\n# drive.mount('/content/drive')",
   "metadata": {
    "id": "RFdglERWDZAj",
    "outputId": "9fc8faca-374e-40b1-e729-9250455ae958",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:00:49.979002Z",
     "iopub.execute_input": "2024-09-28T03:00:49.979340Z",
     "iopub.status.idle": "2024-09-28T03:00:49.983735Z",
     "shell.execute_reply.started": "2024-09-28T03:00:49.979303Z",
     "shell.execute_reply": "2024-09-28T03:00:49.982839Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# !pip uninstall scikit-learn\n!pip install scikit-learn==1.2.2\n!pip install scikeras\n!pip install tensorflow\n\n",
   "metadata": {
    "id": "8TVW18uXEmRi",
    "outputId": "f0a3af4a-e017-4195-c27f-b3f9b81937ec",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:05:48.812161Z",
     "iopub.execute_input": "2024-09-28T03:05:48.813015Z",
     "iopub.status.idle": "2024-09-28T03:06:30.104884Z",
     "shell.execute_reply.started": "2024-09-28T03:05:48.812962Z",
     "shell.execute_reply": "2024-09-28T03:06:30.103536Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.5.0)\nDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.6/9.6 MB\u001B[0m \u001B[31m25.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.5.2\n    Uninstalling scikit-learn-1.5.2:\n      Successfully uninstalled scikit-learn-1.5.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nscikeras 0.13.0 requires scikit-learn>=1.4.2, but you have scikit-learn 1.2.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed scikit-learn-1.2.2\nRequirement already satisfied: scikeras in /opt/conda/lib/python3.10/site-packages (0.13.0)\nRequirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from scikeras) (3.3.3)\nCollecting scikit-learn>=1.4.2 (from scikeras)\n  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (3.11.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.11.0)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->scikeras) (0.3.2)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.1)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\nUsing cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed scikit-learn-1.5.2\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.62.2)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom scikeras.wrappers import KerasRegressor\nfrom sklearn.model_selection import RandomizedSearchCV",
   "metadata": {
    "id": "kjGJnyW9D-0M",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:54:17.479604Z",
     "iopub.execute_input": "2024-09-28T03:54:17.480536Z",
     "iopub.status.idle": "2024-09-28T03:54:17.486626Z",
     "shell.execute_reply.started": "2024-09-28T03:54:17.480489Z",
     "shell.execute_reply": "2024-09-28T03:54:17.485707Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv('/kaggle/input/bitcoin-history1-clean/bitcoin_history1_clean.csv')\ndf = df.set_index('block_time')\n\n# List of fee rate columns that need to be converted from BTC/vB to sat/vB\nfee_rate_columns = ['max_fee_rate', 'avg_fee_rate', 'median_fee_rate', 'fee_rate_10th', 'fee_rate_90th', 'fee_rate_std', 'block_median_fee_rate']\n\n\nfor col in fee_rate_columns:\n    df[col] = df[col] * 10**8\n\n\n# Define features (X) and target (y)\nX = df.drop(columns=['block_median_fee_rate'])\ny = df['block_median_fee_rate']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "metadata": {
    "id": "93c7281d5bf6d67c",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:08:22.496481Z",
     "iopub.execute_input": "2024-09-28T03:08:22.497265Z",
     "iopub.status.idle": "2024-09-28T03:08:22.732585Z",
     "shell.execute_reply.started": "2024-09-28T03:08:22.497222Z",
     "shell.execute_reply": "2024-09-28T03:08:22.731623Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
   "metadata": {
    "id": "9r5aDkCAo13V",
    "outputId": "cc1e2372-4ad0-4d3a-fbaa-b2285caebf7b",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:08:24.014934Z",
     "iopub.execute_input": "2024-09-28T03:08:24.015318Z",
     "iopub.status.idle": "2024-09-28T03:08:24.310960Z",
     "shell.execute_reply.started": "2024-09-28T03:08:24.015282Z",
     "shell.execute_reply": "2024-09-28T03:08:24.309936Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Num GPUs Available:  2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!nvidia-smi\n",
   "metadata": {
    "id": "p7I0krr7rAiK",
    "outputId": "1f3fa1a9-c0f7-4ffb-90b8-82b5812b21e1",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:08:25.639659Z",
     "iopub.execute_input": "2024-09-28T03:08:25.640053Z",
     "iopub.status.idle": "2024-09-28T03:08:26.772256Z",
     "shell.execute_reply.started": "2024-09-28T03:08:25.640014Z",
     "shell.execute_reply": "2024-09-28T03:08:26.770996Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Sat Sep 28 03:08:26 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   32C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 3. LSTM Model without fine tuning\ndef create_lstm_model(input_shape):\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n    model.add(LSTM(units=50))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n# Reshape data for LSTM\nX_train_lstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_test_lstm = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n\nlstm_model = create_lstm_model((X_train_lstm.shape[1], 1))\nlstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=2)\nlstm_y_pred = lstm_model.predict(X_test_lstm)\nresults={}\nresults['LSTM (without fine tuning)'] = {\n    'MAE': mean_absolute_error(y_test, lstm_y_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, lstm_y_pred)),\n    'R²': r2_score(y_test, lstm_y_pred)\n}",
   "metadata": {
    "id": "vPenqylS7rDc",
    "outputId": "54bcfc3e-3a00-4ecd-d6bf-de42e3834946",
    "execution": {
     "iopub.status.busy": "2024-09-28T03:08:27.727793Z",
     "iopub.execute_input": "2024-09-28T03:08:27.728245Z",
     "iopub.status.idle": "2024-09-28T03:09:07.194271Z",
     "shell.execute_reply.started": "2024-09-28T03:08:27.728201Z",
     "shell.execute_reply": "2024-09-28T03:09:07.193446Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1/10\n668/668 - 6s - 10ms/step - loss: 5917.8306\nEpoch 2/10\n668/668 - 3s - 5ms/step - loss: 4805.9033\nEpoch 3/10\n668/668 - 3s - 5ms/step - loss: 4627.1963\nEpoch 4/10\n668/668 - 3s - 5ms/step - loss: 4613.1577\nEpoch 5/10\n668/668 - 3s - 5ms/step - loss: 4612.9082\nEpoch 6/10\n668/668 - 5s - 8ms/step - loss: 3656.9553\nEpoch 7/10\n668/668 - 3s - 5ms/step - loss: 3318.8281\nEpoch 8/10\n668/668 - 3s - 5ms/step - loss: 3272.1143\nEpoch 9/10\n668/668 - 3s - 5ms/step - loss: 3253.9314\nEpoch 10/10\n668/668 - 3s - 5ms/step - loss: 3242.5674\n\u001B[1m167/167\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "results",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T03:09:07.196186Z",
     "iopub.execute_input": "2024-09-28T03:09:07.196933Z",
     "iopub.status.idle": "2024-09-28T03:09:07.204463Z",
     "shell.execute_reply.started": "2024-09-28T03:09:07.196880Z",
     "shell.execute_reply": "2024-09-28T03:09:07.203505Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'LSTM (without fine tuning)': {'MAE': 34.58225648181667,\n  'RMSE': 55.132894410946584,\n  'R²': 0.30463170264614803}}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "1. First Approach: Independent Samples with Multiple Features\nIn this approach, assume that each sample (e.g., each block in your case) is independent, and just reshape the data for LSTM input without considering any sequence.",
   "metadata": {
    "id": "lPCpNMo-o2aN"
   }
  },
  {
   "cell_type": "code",
   "source": "# Normalize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN-LSTM\nX_train_cnn_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\nX_test_cnn_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n\ndef create_cnn_lstm_model(lstm_units=50, conv_filters=64, conv_kernel_size=3, pool_size=2,\n                          dropout_rate=0.2, learning_rate=0.001, optimizer='adam'):\n    model = Sequential()\n    model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel_size, activation='relu',\n                     input_shape=(X_train_cnn_lstm.shape[1], 1)))\n    model.add(MaxPooling1D(pool_size=pool_size))\n    model.add(LSTM(units=lstm_units, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(units=lstm_units//2))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1))\n\n    if optimizer == 'adam':\n        opt = Adam(learning_rate=learning_rate)\n    elif optimizer == 'rmsprop':\n        opt = RMSprop(learning_rate=learning_rate)\n\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    return model\n\n# Create the KerasRegressor\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\ncnn_lstm_regressor = KerasRegressor(model=create_cnn_lstm_model, verbose=0, callbacks=[early_stopping])\n\n# Define the hyperparameter grid\nparam_grid = {\n    'model__lstm_units': [32, 64],\n    'model__conv_filters': [32, 64],\n    'model__conv_kernel_size': [3],\n    'model__pool_size': [2],\n    'model__dropout_rate': [0.1, 0.2],\n    'model__learning_rate': [0.001],\n    'model__optimizer': ['adam'],\n    'batch_size': [32],\n    'epochs': [50]\n}\n\n\n# Initialize and run RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=cnn_lstm_regressor, param_distributions=param_grid,\n                                   n_iter=30, cv=3, verbose=2, random_state=42, n_jobs=1,\n                                   scoring='neg_mean_squared_error')\n\nrandom_search_result = random_search.fit(X_train_cnn_lstm, y_train)\n\n# Print the best parameters and score\nprint(\"Best Parameters: \", random_search_result.best_params_)\nprint(\"Best MSE: \", -random_search_result.best_score_)\n\n# Evaluate on the test set\nbest_cnn_lstm_model = random_search_result.best_estimator_\ncnn_lstm_y_pred = best_cnn_lstm_model.predict(X_test_cnn_lstm)\n\n",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T18:06:44.885794Z",
     "start_time": "2024-09-22T17:34:28.997310Z"
    },
    "id": "9361f82307a7f563",
    "outputId": "bfae59d4-5438-4944-cb18-906c2ce0ae8f",
    "execution": {
     "iopub.status.busy": "2024-09-24T16:30:52.470665Z",
     "iopub.execute_input": "2024-09-24T16:30:52.471043Z",
     "iopub.status.idle": "2024-09-24T17:16:15.737086Z",
     "shell.execute_reply.started": "2024-09-24T16:30:52.471008Z",
     "shell.execute_reply": "2024-09-24T17:16:15.736180Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\nmae = mean_absolute_error(y_test, cnn_lstm_y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, cnn_lstm_y_pred))\nr2 = r2_score(y_test, cnn_lstm_y_pred)\n\nresults['LSTM (tuned)'] = {\n    'MAE': mae,\n    'RMSE': rmse ,\n    'R²': r2\n}\n\n# save the best model\nbest_cnn_lstm_model.model_.save('best_cnn_lstm_model.h5')",
   "metadata": {
    "id": "J9kCGMfW1XSP",
    "execution": {
     "iopub.status.busy": "2024-09-24T17:16:15.738800Z",
     "iopub.execute_input": "2024-09-24T17:16:15.739148Z",
     "iopub.status.idle": "2024-09-24T17:16:15.798669Z",
     "shell.execute_reply.started": "2024-09-24T17:16:15.739114Z",
     "shell.execute_reply": "2024-09-24T17:16:15.797843Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results",
   "metadata": {
    "id": "XMmNZCpg9lUi",
    "execution": {
     "iopub.status.busy": "2024-09-24T17:16:15.799797Z",
     "iopub.execute_input": "2024-09-24T17:16:15.800125Z",
     "iopub.status.idle": "2024-09-24T17:16:15.806902Z",
     "shell.execute_reply.started": "2024-09-24T17:16:15.800089Z",
     "shell.execute_reply": "2024-09-24T17:16:15.806002Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Normalize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN-LSTM\nX_train_cnn_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\nX_test_cnn_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n\ndef create_cnn_lstm_model(lstm_units=50, conv_filters=64, conv_kernel_size=3, pool_size=2,\n                          dropout_rate=0.2, learning_rate=0.001, optimizer='adam'):\n    model = Sequential()\n    model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel_size, activation='relu',\n                     input_shape=(X_train_cnn_lstm.shape[1], 1)))\n    model.add(MaxPooling1D(pool_size=pool_size))\n    model.add(LSTM(units=lstm_units, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(units=lstm_units//2))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1))\n\n    if optimizer == 'adam':\n        opt = Adam(learning_rate=learning_rate)\n    elif optimizer == 'rmsprop':\n        opt = RMSprop(learning_rate=learning_rate)\n\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    return model\n\n# Create the KerasRegressor\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\ncnn_lstm_regressor = KerasRegressor(model=create_cnn_lstm_model, verbose=0, callbacks=[early_stopping])\n\n# Define the hyperparameter grid\nparam_grid = {\n    'model__lstm_units': [64],  # Fixed for now\n    'model__conv_filters': [64],  # Fixed for now\n    'model__conv_kernel_size': [3],\n    'model__pool_size': [2, 3],\n    'model__dropout_rate': [0.1, 0.2, 0.3],\n    'model__learning_rate': [0.001, 0.0005, 0.0001],\n    'model__optimizer': ['adam', 'rmsprop'],\n    'batch_size': [32, 64],\n    'epochs': [50,100]\n}\n\n\n# Initialize and run RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=cnn_lstm_regressor, param_distributions=param_grid,\n                                   n_iter=30, cv=3, verbose=2, random_state=42, n_jobs=1,\n                                   scoring='neg_mean_squared_error')\n\nrandom_search_result = random_search.fit(X_train_cnn_lstm, y_train)\n\n# Print the best parameters and score\nprint(\"Best Parameters: \", random_search_result.best_params_)\nprint(\"Best MSE: \", -random_search_result.best_score_)\n\n# Evaluate on the test set\nbest_cnn_lstm_model = random_search_result.best_estimator_\ncnn_lstm_y_pred = best_cnn_lstm_model.predict(X_test_cnn_lstm)\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-24T17:27:19.300182Z",
     "iopub.execute_input": "2024-09-24T17:27:19.300928Z",
     "iopub.status.idle": "2024-09-24T20:29:06.496833Z",
     "shell.execute_reply.started": "2024-09-24T17:27:19.300886Z",
     "shell.execute_reply": "2024-09-24T20:29:06.495866Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\nmae = mean_absolute_error(y_test, cnn_lstm_y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, cnn_lstm_y_pred))\nr2 = r2_score(y_test, cnn_lstm_y_pred)\n\nresults['LSTM (tuned1)'] = {\n    'MAE': mae,\n    'RMSE': rmse ,\n    'R²': r2\n}\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-24T20:29:06.498706Z",
     "iopub.execute_input": "2024-09-24T20:29:06.499041Z",
     "iopub.status.idle": "2024-09-24T20:29:06.507405Z",
     "shell.execute_reply.started": "2024-09-24T20:29:06.498980Z",
     "shell.execute_reply": "2024-09-24T20:29:06.506473Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-24T20:29:06.508450Z",
     "iopub.execute_input": "2024-09-24T20:29:06.508746Z",
     "iopub.status.idle": "2024-09-24T20:29:06.525932Z",
     "shell.execute_reply.started": "2024-09-24T20:29:06.508712Z",
     "shell.execute_reply": "2024-09-24T20:29:06.525074Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Normalize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN-LSTM\nX_train_cnn_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\nX_test_cnn_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n\ndef create_cnn_lstm_model(lstm_units=50, conv_filters=64, conv_kernel_size=3, pool_size=2,\n                          dropout_rate=0.2, learning_rate=0.001, optimizer='adam'):\n    model = Sequential()\n    model.add(Conv1D(filters=conv_filters, kernel_size=conv_kernel_size, activation='relu',\n                     input_shape=(X_train_cnn_lstm.shape[1], 1)))\n    model.add(MaxPooling1D(pool_size=pool_size))\n    model.add(LSTM(units=lstm_units, return_sequences=True))\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(units=lstm_units//2))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1)) \n\n    if optimizer == 'adam':\n        opt = Adam(learning_rate=learning_rate)\n    elif optimizer == 'rmsprop':\n        opt = RMSprop(learning_rate=learning_rate)\n\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    return model\n\n# Create the KerasRegressor\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\ncnn_lstm_regressor = KerasRegressor(model=create_cnn_lstm_model, verbose=0, callbacks=[early_stopping])\n\n# Define the hyperparameter grid\nparam_grid = {\n    'model__lstm_units': [64, 128, 256],  \n    'model__conv_filters': [64, 128, 256], \n    'model__conv_kernel_size': [3],\n    'model__pool_size': [3,4],\n    'model__dropout_rate': [0.2],\n    'model__learning_rate': [0.001,0.005,0.01],\n    'model__optimizer': [ 'rmsprop'],\n    'batch_size': [16,32],\n    'epochs': [50,100]\n}\n\n\n# Initialize and run RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=cnn_lstm_regressor, param_distributions=param_grid,\n                                   n_iter=30, cv=3, verbose=2, random_state=42, n_jobs=1,\n                                   scoring='neg_mean_squared_error')\n\nrandom_search_result = random_search.fit(X_train_cnn_lstm, y_train)\n\n# Print the best parameters and score\nprint(\"Best Parameters: \", random_search_result.best_params_)\nprint(\"Best MSE: \", -random_search_result.best_score_)\n\n# Evaluate on the test set\nbest_cnn_lstm_model = random_search_result.best_estimator_\ncnn_lstm_y_pred = best_cnn_lstm_model.predict(X_test_cnn_lstm)\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-24T20:44:06.589921Z",
     "iopub.execute_input": "2024-09-24T20:44:06.590878Z",
     "iopub.status.idle": "2024-09-25T01:10:05.839633Z",
     "shell.execute_reply.started": "2024-09-24T20:44:06.590839Z",
     "shell.execute_reply": "2024-09-25T01:10:05.838636Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define the hyperparameter grid best\nparam_grid = {\n    'model__lstm_units': [128], \n    'model__conv_filters': [128],\n    'model__conv_kernel_size': [3],\n    'model__pool_size': [3],\n    'model__dropout_rate': [0.2],\n    'model__learning_rate': [0.001],\n    'model__optimizer': [ 'rmsprop'], \n    'batch_size': [32],\n    'epochs': [100]\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "LSTM (tuned2) fine tuning",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense,LayerNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import mean_squared_error\n\n# Normalize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN-LSTM\nX_train_cnn_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\nX_test_cnn_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n\n# Define the CNN-LSTM model using the best hyperparameters\ndef create_cnn_lstm_model():\n    model = Sequential()\n    \n    # First Conv1D layer\n    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(X_train_cnn_lstm.shape[1], 1)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=3))\n    \n#     # Second Conv1D layer\n#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n#     model.add(MaxPooling1D(pool_size=3))\n    \n    # First LSTM layer\n    model.add(LSTM(units=128, return_sequences=True))\n    model.add(LayerNormalization())\n    model.add(Dropout(0.2))\n    \n    # Second LSTM layer\n    model.add(LSTM(units=64, return_sequences=False))\n    model.add(LayerNormalization())\n    model.add(Dropout(0.2))\n    \n    # Output layer\n    model.add(Dense(1))\n    \n    # Optimizer\n    opt = RMSprop(learning_rate=0.001)\n    \n    # Compile the model\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    \n    return model\n\n# Instantiate the model\ncnn_lstm_model = create_cnn_lstm_model()\n\n# Early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Train the model\ncnn_lstm_model.fit(X_train_cnn_lstm, y_train, \n                   validation_split=0.2, \n                   epochs=100, \n                   batch_size=32, \n                   callbacks=[early_stopping], \n                   verbose=2)\n\n# Evaluate on the test set\ncnn_lstm_y_pred = cnn_lstm_model.predict(X_test_cnn_lstm)\n\n\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:46:38.442178Z",
     "iopub.execute_input": "2024-09-28T04:46:38.443251Z",
     "iopub.status.idle": "2024-09-28T04:49:28.498363Z",
     "shell.execute_reply.started": "2024-09-28T04:46:38.443198Z",
     "shell.execute_reply": "2024-09-28T04:49:28.497164Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/100\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "534/534 - 5s - 10ms/step - loss: 4867.4746 - val_loss: 3026.1741\nEpoch 2/100\n534/534 - 3s - 5ms/step - loss: 2363.5022 - val_loss: 1863.6954\nEpoch 3/100\n534/534 - 3s - 5ms/step - loss: 1668.5961 - val_loss: 1389.2795\nEpoch 4/100\n534/534 - 3s - 5ms/step - loss: 1351.8878 - val_loss: 1201.0652\nEpoch 5/100\n534/534 - 3s - 5ms/step - loss: 1159.7240 - val_loss: 1058.0433\nEpoch 6/100\n534/534 - 3s - 6ms/step - loss: 1039.9971 - val_loss: 963.2625\nEpoch 7/100\n534/534 - 3s - 5ms/step - loss: 964.7334 - val_loss: 905.6763\nEpoch 8/100\n534/534 - 3s - 5ms/step - loss: 921.7800 - val_loss: 857.5228\nEpoch 9/100\n534/534 - 3s - 5ms/step - loss: 862.7465 - val_loss: 819.1367\nEpoch 10/100\n534/534 - 3s - 5ms/step - loss: 824.4869 - val_loss: 782.0953\nEpoch 11/100\n534/534 - 3s - 5ms/step - loss: 766.6662 - val_loss: 836.4711\nEpoch 12/100\n534/534 - 3s - 5ms/step - loss: 747.6499 - val_loss: 726.2568\nEpoch 13/100\n534/534 - 3s - 6ms/step - loss: 704.7455 - val_loss: 716.9762\nEpoch 14/100\n534/534 - 3s - 5ms/step - loss: 685.3763 - val_loss: 671.4745\nEpoch 15/100\n534/534 - 3s - 5ms/step - loss: 664.1977 - val_loss: 691.1323\nEpoch 16/100\n534/534 - 3s - 6ms/step - loss: 622.9020 - val_loss: 691.0903\nEpoch 17/100\n534/534 - 3s - 6ms/step - loss: 617.5918 - val_loss: 690.8583\nEpoch 18/100\n534/534 - 3s - 5ms/step - loss: 590.7719 - val_loss: 643.5060\nEpoch 19/100\n534/534 - 3s - 5ms/step - loss: 575.1838 - val_loss: 665.7130\nEpoch 20/100\n534/534 - 3s - 5ms/step - loss: 550.1165 - val_loss: 638.0501\nEpoch 21/100\n534/534 - 3s - 5ms/step - loss: 547.2656 - val_loss: 633.7162\nEpoch 22/100\n534/534 - 3s - 5ms/step - loss: 502.7555 - val_loss: 611.1595\nEpoch 23/100\n534/534 - 3s - 5ms/step - loss: 511.0795 - val_loss: 633.4419\nEpoch 24/100\n534/534 - 3s - 5ms/step - loss: 488.6388 - val_loss: 608.6285\nEpoch 25/100\n534/534 - 3s - 5ms/step - loss: 487.5601 - val_loss: 630.1483\nEpoch 26/100\n534/534 - 3s - 6ms/step - loss: 490.2585 - val_loss: 603.5106\nEpoch 27/100\n534/534 - 3s - 6ms/step - loss: 468.1529 - val_loss: 607.0268\nEpoch 28/100\n534/534 - 3s - 5ms/step - loss: 454.6734 - val_loss: 676.9482\nEpoch 29/100\n534/534 - 3s - 5ms/step - loss: 445.5551 - val_loss: 571.9272\nEpoch 30/100\n534/534 - 3s - 5ms/step - loss: 437.7650 - val_loss: 530.9053\nEpoch 31/100\n534/534 - 3s - 5ms/step - loss: 433.8202 - val_loss: 529.7075\nEpoch 32/100\n534/534 - 3s - 5ms/step - loss: 406.6062 - val_loss: 541.6265\nEpoch 33/100\n534/534 - 3s - 5ms/step - loss: 411.8207 - val_loss: 528.3660\nEpoch 34/100\n534/534 - 3s - 6ms/step - loss: 388.8052 - val_loss: 552.2297\nEpoch 35/100\n534/534 - 3s - 5ms/step - loss: 399.2679 - val_loss: 544.3914\nEpoch 36/100\n534/534 - 3s - 5ms/step - loss: 391.7309 - val_loss: 543.8022\nEpoch 37/100\n534/534 - 3s - 5ms/step - loss: 377.0452 - val_loss: 542.4554\nEpoch 38/100\n534/534 - 3s - 6ms/step - loss: 381.5562 - val_loss: 556.6804\nEpoch 39/100\n534/534 - 3s - 5ms/step - loss: 366.1196 - val_loss: 501.2584\nEpoch 40/100\n534/534 - 3s - 5ms/step - loss: 366.1909 - val_loss: 543.1270\nEpoch 41/100\n534/534 - 3s - 5ms/step - loss: 361.1821 - val_loss: 538.3461\nEpoch 42/100\n534/534 - 3s - 5ms/step - loss: 360.3451 - val_loss: 543.7759\nEpoch 43/100\n534/534 - 3s - 5ms/step - loss: 347.3997 - val_loss: 512.7344\nEpoch 44/100\n534/534 - 3s - 5ms/step - loss: 363.6126 - val_loss: 512.2944\nEpoch 45/100\n534/534 - 3s - 5ms/step - loss: 343.0479 - val_loss: 526.7287\nEpoch 46/100\n534/534 - 3s - 5ms/step - loss: 336.3159 - val_loss: 482.4152\nEpoch 47/100\n534/534 - 3s - 5ms/step - loss: 322.3777 - val_loss: 464.2536\nEpoch 48/100\n534/534 - 3s - 5ms/step - loss: 333.2492 - val_loss: 494.7672\nEpoch 49/100\n534/534 - 3s - 6ms/step - loss: 324.9557 - val_loss: 495.0742\nEpoch 50/100\n534/534 - 3s - 5ms/step - loss: 312.9910 - val_loss: 502.6078\nEpoch 51/100\n534/534 - 3s - 5ms/step - loss: 314.7236 - val_loss: 506.4137\nEpoch 52/100\n534/534 - 3s - 5ms/step - loss: 307.9710 - val_loss: 543.0210\nEpoch 53/100\n534/534 - 3s - 5ms/step - loss: 312.8137 - val_loss: 578.6579\nEpoch 54/100\n534/534 - 3s - 6ms/step - loss: 309.4737 - val_loss: 492.2617\nEpoch 55/100\n534/534 - 3s - 5ms/step - loss: 298.5382 - val_loss: 524.1625\nEpoch 56/100\n534/534 - 3s - 5ms/step - loss: 305.4030 - val_loss: 538.3731\nEpoch 57/100\n534/534 - 3s - 5ms/step - loss: 295.6265 - val_loss: 522.2574\n\u001B[1m167/167\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\nmae = mean_absolute_error(y_test, cnn_lstm_y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, cnn_lstm_y_pred))\nr2 = r2_score(y_test, cnn_lstm_y_pred)\n\nresults['LSTM (tuned2)'] = {\n    'MAE': mae,\n    'RMSE': rmse ,\n    'R²': r2\n}\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:49:28.500878Z",
     "iopub.execute_input": "2024-09-28T04:49:28.501201Z",
     "iopub.status.idle": "2024-09-28T04:49:28.511558Z",
     "shell.execute_reply.started": "2024-09-28T04:49:28.501167Z",
     "shell.execute_reply": "2024-09-28T04:49:28.510503Z"
    },
    "trusted": true
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:49:28.512829Z",
     "iopub.execute_input": "2024-09-28T04:49:28.513134Z",
     "iopub.status.idle": "2024-09-28T04:49:28.525603Z",
     "shell.execute_reply.started": "2024-09-28T04:49:28.513100Z",
     "shell.execute_reply": "2024-09-28T04:49:28.524778Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": [
    {
     "execution_count": 43,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'LSTM (without fine tuning)': {'MAE': 34.58225648181667,\n  'RMSE': 55.132894410946584,\n  'R²': 0.30463170264614803},\n 'LSTM (tuned3)': {'MAE': 9.33564540674665,\n  'RMSE': 18.266595049879566,\n  'R²': 0.9236676728626771},\n 'LSTM (tuned2)': {'MAE': 10.182954115933548,\n  'RMSE': 19.686484439226202,\n  'R²': 0.9113396101606775}}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Best model\n",
    "# Normalize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN-LSTM\n",
    "X_train_cnn_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_cnn_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Define the CNN-LSTM model using the best hyperparameters\n",
    "def create_cnn_lstm_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu',\n",
    "                     input_shape=(X_train_cnn_lstm.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Optimizer (using the best learning rate and RMSProp optimizer)\n",
    "    opt = RMSprop(learning_rate=0.001)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "cnn_lstm_model = create_cnn_lstm_model()\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "cnn_lstm_model.fit(X_train_cnn_lstm, y_train, \n",
    "                   validation_split=0.2, \n",
    "                   epochs=100, \n",
    "                   batch_size=32, \n",
    "                   callbacks=[early_stopping], \n",
    "                   verbose=2)\n",
    "\n",
    "# Evaluate on the test set\n",
    "cnn_lstm_y_pred = cnn_lstm_model.predict(X_test_cnn_lstm)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:02:12.437470Z",
     "iopub.execute_input": "2024-09-28T04:02:12.438650Z",
     "iopub.status.idle": "2024-09-28T04:06:04.818582Z",
     "shell.execute_reply.started": "2024-09-28T04:02:12.438602Z",
     "shell.execute_reply": "2024-09-28T04:06:04.817725Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/100\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "534/534 - 4s - 8ms/step - loss: 5566.6694 - val_loss: 4458.9009\nEpoch 2/100\n534/534 - 3s - 5ms/step - loss: 3863.8911 - val_loss: 3218.9304\nEpoch 3/100\n534/534 - 2s - 5ms/step - loss: 2892.6917 - val_loss: 2507.4495\nEpoch 4/100\n534/534 - 3s - 5ms/step - loss: 2355.0793 - val_loss: 2081.6624\nEpoch 5/100\n534/534 - 2s - 5ms/step - loss: 1983.8469 - val_loss: 1814.8898\nEpoch 6/100\n534/534 - 3s - 5ms/step - loss: 1726.3030 - val_loss: 1545.6713\nEpoch 7/100\n534/534 - 3s - 5ms/step - loss: 1473.6467 - val_loss: 1352.4801\nEpoch 8/100\n534/534 - 3s - 5ms/step - loss: 1308.9408 - val_loss: 1255.3600\nEpoch 9/100\n534/534 - 2s - 5ms/step - loss: 1148.1788 - val_loss: 1133.8040\nEpoch 10/100\n534/534 - 3s - 5ms/step - loss: 1041.5220 - val_loss: 1024.7556\nEpoch 11/100\n534/534 - 2s - 5ms/step - loss: 966.8279 - val_loss: 933.0691\nEpoch 12/100\n534/534 - 2s - 5ms/step - loss: 902.5605 - val_loss: 947.4424\nEpoch 13/100\n534/534 - 2s - 5ms/step - loss: 841.0476 - val_loss: 776.9385\nEpoch 14/100\n534/534 - 3s - 5ms/step - loss: 801.4478 - val_loss: 861.4741\nEpoch 15/100\n534/534 - 2s - 5ms/step - loss: 777.2856 - val_loss: 1007.9588\nEpoch 16/100\n534/534 - 2s - 5ms/step - loss: 739.0419 - val_loss: 804.6800\nEpoch 17/100\n534/534 - 3s - 5ms/step - loss: 716.1579 - val_loss: 736.1555\nEpoch 18/100\n534/534 - 3s - 5ms/step - loss: 676.4553 - val_loss: 824.6490\nEpoch 19/100\n534/534 - 2s - 5ms/step - loss: 652.6121 - val_loss: 672.5287\nEpoch 20/100\n534/534 - 3s - 5ms/step - loss: 616.9166 - val_loss: 700.0478\nEpoch 21/100\n534/534 - 3s - 5ms/step - loss: 612.8893 - val_loss: 688.8237\nEpoch 22/100\n534/534 - 3s - 5ms/step - loss: 592.7010 - val_loss: 643.8076\nEpoch 23/100\n534/534 - 3s - 5ms/step - loss: 575.0583 - val_loss: 715.7952\nEpoch 24/100\n534/534 - 3s - 5ms/step - loss: 552.2753 - val_loss: 725.3073\nEpoch 25/100\n534/534 - 3s - 5ms/step - loss: 544.6267 - val_loss: 605.8205\nEpoch 26/100\n534/534 - 3s - 5ms/step - loss: 536.8122 - val_loss: 638.2311\nEpoch 27/100\n534/534 - 3s - 5ms/step - loss: 518.4020 - val_loss: 540.0441\nEpoch 28/100\n534/534 - 2s - 5ms/step - loss: 499.9916 - val_loss: 600.9097\nEpoch 29/100\n534/534 - 3s - 5ms/step - loss: 495.8883 - val_loss: 599.7467\nEpoch 30/100\n534/534 - 3s - 5ms/step - loss: 482.0347 - val_loss: 555.6682\nEpoch 31/100\n534/534 - 2s - 5ms/step - loss: 480.2289 - val_loss: 574.3653\nEpoch 32/100\n534/534 - 3s - 5ms/step - loss: 468.9999 - val_loss: 632.5814\nEpoch 33/100\n534/534 - 2s - 5ms/step - loss: 452.8408 - val_loss: 513.4208\nEpoch 34/100\n534/534 - 3s - 5ms/step - loss: 462.9251 - val_loss: 605.0918\nEpoch 35/100\n534/534 - 2s - 5ms/step - loss: 434.3519 - val_loss: 606.1002\nEpoch 36/100\n534/534 - 2s - 5ms/step - loss: 449.9592 - val_loss: 578.5262\nEpoch 37/100\n534/534 - 2s - 5ms/step - loss: 421.4935 - val_loss: 571.9724\nEpoch 38/100\n534/534 - 3s - 5ms/step - loss: 431.8621 - val_loss: 539.5598\nEpoch 39/100\n534/534 - 3s - 5ms/step - loss: 404.8780 - val_loss: 523.4103\nEpoch 40/100\n534/534 - 3s - 5ms/step - loss: 410.2058 - val_loss: 535.6922\nEpoch 41/100\n534/534 - 2s - 5ms/step - loss: 401.0153 - val_loss: 551.3381\nEpoch 42/100\n534/534 - 3s - 5ms/step - loss: 399.0240 - val_loss: 474.7844\nEpoch 43/100\n534/534 - 3s - 5ms/step - loss: 400.3555 - val_loss: 454.4811\nEpoch 44/100\n534/534 - 2s - 5ms/step - loss: 383.3741 - val_loss: 555.6409\nEpoch 45/100\n534/534 - 3s - 5ms/step - loss: 383.6002 - val_loss: 510.3670\nEpoch 46/100\n534/534 - 3s - 5ms/step - loss: 377.1052 - val_loss: 504.9269\nEpoch 47/100\n534/534 - 3s - 5ms/step - loss: 374.9797 - val_loss: 494.0084\nEpoch 48/100\n534/534 - 2s - 5ms/step - loss: 372.4782 - val_loss: 458.7087\nEpoch 49/100\n534/534 - 2s - 5ms/step - loss: 352.5275 - val_loss: 518.3122\nEpoch 50/100\n534/534 - 3s - 5ms/step - loss: 367.1783 - val_loss: 492.4010\nEpoch 51/100\n534/534 - 2s - 5ms/step - loss: 340.7670 - val_loss: 452.8157\nEpoch 52/100\n534/534 - 2s - 5ms/step - loss: 350.7079 - val_loss: 531.7245\nEpoch 53/100\n534/534 - 3s - 5ms/step - loss: 349.7731 - val_loss: 578.1009\nEpoch 54/100\n534/534 - 3s - 5ms/step - loss: 345.3561 - val_loss: 452.8481\nEpoch 55/100\n534/534 - 2s - 5ms/step - loss: 337.3804 - val_loss: 442.9577\nEpoch 56/100\n534/534 - 2s - 5ms/step - loss: 337.9528 - val_loss: 468.3939\nEpoch 57/100\n534/534 - 3s - 5ms/step - loss: 340.0820 - val_loss: 428.7299\nEpoch 58/100\n534/534 - 3s - 5ms/step - loss: 320.5636 - val_loss: 447.3053\nEpoch 59/100\n534/534 - 2s - 5ms/step - loss: 325.1364 - val_loss: 461.4661\nEpoch 60/100\n534/534 - 2s - 5ms/step - loss: 316.3784 - val_loss: 498.9733\nEpoch 61/100\n534/534 - 3s - 5ms/step - loss: 324.6037 - val_loss: 481.9674\nEpoch 62/100\n534/534 - 2s - 5ms/step - loss: 306.6651 - val_loss: 415.7605\nEpoch 63/100\n534/534 - 2s - 5ms/step - loss: 315.4264 - val_loss: 478.0910\nEpoch 64/100\n534/534 - 2s - 5ms/step - loss: 303.9073 - val_loss: 451.0072\nEpoch 65/100\n534/534 - 3s - 5ms/step - loss: 294.2840 - val_loss: 415.0363\nEpoch 66/100\n534/534 - 2s - 5ms/step - loss: 304.2237 - val_loss: 451.6881\nEpoch 67/100\n534/534 - 2s - 5ms/step - loss: 297.7863 - val_loss: 426.4661\nEpoch 68/100\n534/534 - 2s - 5ms/step - loss: 308.5316 - val_loss: 448.4228\nEpoch 69/100\n534/534 - 3s - 5ms/step - loss: 291.6942 - val_loss: 494.5511\nEpoch 70/100\n534/534 - 3s - 5ms/step - loss: 285.2544 - val_loss: 477.3243\nEpoch 71/100\n534/534 - 3s - 5ms/step - loss: 280.1817 - val_loss: 438.4257\nEpoch 72/100\n534/534 - 2s - 5ms/step - loss: 292.5096 - val_loss: 406.1321\nEpoch 73/100\n534/534 - 3s - 5ms/step - loss: 294.5481 - val_loss: 496.9839\nEpoch 74/100\n534/534 - 2s - 5ms/step - loss: 283.7626 - val_loss: 417.7380\nEpoch 75/100\n534/534 - 2s - 5ms/step - loss: 276.7645 - val_loss: 437.3343\nEpoch 76/100\n534/534 - 2s - 5ms/step - loss: 274.1130 - val_loss: 415.0251\nEpoch 77/100\n534/534 - 3s - 5ms/step - loss: 281.0922 - val_loss: 453.8882\nEpoch 78/100\n534/534 - 2s - 5ms/step - loss: 268.1271 - val_loss: 458.9989\nEpoch 79/100\n534/534 - 2s - 5ms/step - loss: 278.6155 - val_loss: 432.6880\nEpoch 80/100\n534/534 - 3s - 5ms/step - loss: 271.5277 - val_loss: 438.0318\nEpoch 81/100\n534/534 - 3s - 5ms/step - loss: 271.5324 - val_loss: 370.3554\nEpoch 82/100\n534/534 - 3s - 5ms/step - loss: 260.2004 - val_loss: 403.8518\nEpoch 83/100\n534/534 - 3s - 5ms/step - loss: 262.8036 - val_loss: 384.7448\nEpoch 84/100\n534/534 - 2s - 5ms/step - loss: 263.5944 - val_loss: 459.6500\nEpoch 85/100\n534/534 - 3s - 5ms/step - loss: 265.5905 - val_loss: 443.8923\nEpoch 86/100\n534/534 - 2s - 5ms/step - loss: 264.7212 - val_loss: 425.3222\nEpoch 87/100\n534/534 - 2s - 5ms/step - loss: 254.9848 - val_loss: 386.2017\nEpoch 88/100\n534/534 - 2s - 5ms/step - loss: 255.4960 - val_loss: 454.2298\nEpoch 89/100\n534/534 - 3s - 5ms/step - loss: 253.1639 - val_loss: 378.7525\nEpoch 90/100\n534/534 - 2s - 5ms/step - loss: 245.6364 - val_loss: 405.6002\nEpoch 91/100\n534/534 - 2s - 5ms/step - loss: 250.1741 - val_loss: 388.7153\n\u001B[1m167/167\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Calculate performance metrics\nmae = mean_absolute_error(y_test, cnn_lstm_y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, cnn_lstm_y_pred))\nr2 = r2_score(y_test, cnn_lstm_y_pred)\n\nresults['LSTM (tuned3)'] = {\n    'MAE': mae,\n    'RMSE': rmse ,\n    'R²': r2\n}\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:06:04.820643Z",
     "iopub.execute_input": "2024-09-28T04:06:04.820976Z",
     "iopub.status.idle": "2024-09-28T04:06:04.830008Z",
     "shell.execute_reply.started": "2024-09-28T04:06:04.820940Z",
     "shell.execute_reply": "2024-09-28T04:06:04.829063Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "results",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-28T04:06:04.831251Z",
     "iopub.execute_input": "2024-09-28T04:06:04.831841Z",
     "iopub.status.idle": "2024-09-28T04:06:04.840379Z",
     "shell.execute_reply.started": "2024-09-28T04:06:04.831794Z",
     "shell.execute_reply": "2024-09-28T04:06:04.839519Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'LSTM (without fine tuning)': {'MAE': 34.58225648181667,\n  'RMSE': 55.132894410946584,\n  'R²': 0.30463170264614803},\n 'LSTM (tuned3)': {'MAE': 9.33564540674665,\n  'RMSE': 18.266595049879566,\n  'R²': 0.9236676728626771},\n 'LSTM (tuned2)': {'MAE': 10.2362190709499,\n  'RMSE': 18.56297168570665,\n  'R²': 0.921170584707235}}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
